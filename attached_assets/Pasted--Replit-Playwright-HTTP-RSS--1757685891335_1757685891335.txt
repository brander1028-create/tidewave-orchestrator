한줄 결론: Replit에선 Playwright가 잘 안 돌아가니, “HTTP 전용(+RSS)”으로 확정하고 실행 가능한 수락 기준과 테스트 절차를 프롬프트에 못 박아야 합니다. 키(오픈API)가 없을 때도 시드 URL로 돌아가는 Fallback을 반드시 넣으라고 지시하세요.

왜 계속 실패하나(핵심 3가지)

환경 불일치: Replit에서 Playwright 런타임/헤드리스 브라우저 의존 → 실패. “HTTP로 바꾼다” 말만 하고 실제 코드/의존성/임포트가 안 바뀐 경우가 많음.

키 의존: 네이버 오픈API 키 없으면 “발견/순위 확인”이 막힘 → 코드가 Graceful Fallback(수동 시드·순위=NA)을 못해 전체가 실패로 끝남.

검증 부재: “완벽” 메시지와 달리 CSV 행 수·샘플 출력·단위 테스트가 없어 거짓 양성(phantom success).

바로 붙여 쓸 “교정 프롬프트”(Tidewave/Replit Agent용)

아래 블록을 그대로 붙여주세요. (거짓 성공 금지, 수락 기준·출력물·테스트를 강제)

You are fixing a crawler to run reliably on Replit (no headless browsers).
STRICT REQUIREMENTS — do ALL of the following:

1) Remove Playwright completely:
   - Delete any playwright imports/calls.
   - In requirements.txt, REMOVE playwright. ADD: requests, beautifulsoup4, lxml, pandas, python-dotenv, httpx.
   - Ensure a clean install step runs.

2) Switch to HTTP-only collection with graceful fallbacks:
   - Discovery:
     a) If NAVER_OPENAPI_ID/SECRET exist ⇒ use Naver blog search API.
     b) ELSE use a hard-coded seed list (accept from CLI: --seeds "https://blog.naver.com/USER1 https://blog.naver.com/USER2").
   - Recent posts (per blog):
     PRIORITY ORDER:
       i) Try RSS: https://rss.blog.naver.com/{BLOG_ID}.xml
       ii) If RSS unavailable, fetch https://m.blog.naver.com/{BLOG_ID} with requests and parse anchors containing "PostView".
     Parse up to N=10 posts (title, url, published_at if present).
     Respect robots/terms; skip login/neighbor-only/adult gates.

3) Keyword extraction:
   - Simple n-gram (1–3) over titles.
   - If no Search Ads key available, rank by frequency; else (if keys present) weight by monthly volume.

4) SERP check:
   - If NAVER_OPENAPI_ID/SECRET present ⇒ query blog search API and return the first-page rank of links containing the blog_id.
   - ELSE set rank to NA (do NOT fail the pipeline).

5) Output (must always be produced even with partial data):
   - out/blogs.csv: blog_id, blog_url, gathered_posts, last_checked
   - out/results.csv: blog_id, post_url, post_title, published_at, top3_keywords(json), serp_mobile_rank, checked_at
   - Print to console: 
       * total blogs discovered
       * first 3 rows of each CSV (pandas .head()).

6) Acceptance tests (HARD FAIL if not met):
   - If seeds are provided, process them even with NO API KEYS.
   - At least 5 posts collected for ≥3 blogs, OR clearly print "PARTIAL: X blogs met threshold" but still write CSVs.
   - No references to Playwright remain in code or requirements.

7) CLI examples (must work):
   - With seeds, no keys:
     python run_mvp.py --recent 10 --out out --seeds "https://blog.naver.com/USER1 https://blog.naver.com/USER2"
   - With keys:
     python run_mvp.py --keyword "홍삼스틱" --blogs 10 --recent 10 --out out

8) Logging:
   - For each blog: show which path used (RSS or HTML), and how many posts found.
   - Backoff on 429/5xx with random sleep 1–3s. Set UA + Accept-Language.

Do not claim success until:
   - Both CSV files exist and have ≥1 row each (except headers).
   - Console shows .head() previews.

시드가 필요한 이유(중요)

키 없이도 즉시 돌아가도록 해야 “0건 → 전체 실패”를 막습니다. 최소 2~3개 블로그 URL을 --seeds로 공급하면 파이프라인·CSV·키워드 추출까지 검증 가능합니다.

예시 시드(형식만 예):
--seeds "https://blog.naver.com/USER1 https://blog.naver.com/USER2"

작업 확인용 체크리스트(짧게)

 requirements.txt에서 playwright 제거, requests/bs4/lxml 추가

 recent 모듈이 RSS → HTML 파싱 순으로 동작

 키 미보유 시에도 out/blogs.csv, out/results.csv 생성

 콘솔에 발견/수집 개수와 CSV .head() 샘플 출력

 “완벽” 같은 문구 대신 수락 기준 충족 여부를 숫자로 보고

실패가 계속되면 마지막 카드

RSS만 먼저 완성 → 90% 블로그는 RSS로 최신 글 목록 확보가 더 쉽고 안정적입니다.

그 다음에만 HTML 파싱(모바일 뷰) 추가. 이렇게 단계적으로 요구하세요.

원하면 위 프롬프트에 맞춰 **HTTP 전용 코드 스켈레톤(Playwright 0%)**으로 캔버스 문서 업데이트해 드릴게요.