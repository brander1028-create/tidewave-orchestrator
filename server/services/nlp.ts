import { listKeywords } from '../store/keywords';

export interface KeywordCandidate {
  keyword: string;
  frequency: number;
  score: number;
  volume?: number;
  grade?: string;
  isFromDB?: boolean;
}

export class NLPService {
  private stopWords = new Set([
    // Korean stopwords - basic particles
    'Ïù¥', 'Í∑∏', 'Ï†Ä', 'Í≤É', 'Îì§', 'Ïùò', 'Í∞Ä', 'ÏùÑ', 'Î•º', 'Ïóê', 'ÏôÄ', 'Í≥º', 'ÎèÑ', 'Îßå', 'ÎèÑ', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'Î°ú', 'Ïù¥Îã§', 'ÏûàÎã§', 'ÌïòÎã§',
    'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü∞Îç∞', 'ÌïòÏßÄÎßå', 'Í∑∏Îü¨ÎÇò', 'ÎòêÌïú', 'Îòê', 'Í∑∏ÎûòÏÑú', 'Îî∞ÎùºÏÑú', 'Ï¶â', 'ÏòàÎ•º Îì§Ïñ¥', 'ÎïåÎ¨∏Ïóê', 'ÏúÑÌï¥', 'ÌÜµÌï¥', 'ÎåÄÌï¥',
    'Ï†ïÎßê', 'Îß§Ïö∞', 'ÎÑàÎ¨¥', 'ÏïÑÏ£º', 'Ï†ïÎßêÎ°ú', 'Ìï≠ÏÉÅ', 'Îäò', 'ÏûêÏ£º', 'Í∞ÄÎÅî', 'ÎïåÎ°úÎäî', 'Ïù¥Î≤à', 'Îã§Ïùå', 'ÏßÄÎÇú', 'Ïò§Îäò', 'Ïñ¥Ï†ú', 'ÎÇ¥Ïùº',
    'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞', 'Ïù¥Í≥≥', 'Í∑∏Í≥≥', 'Ï†ÄÍ≥≥', 'ÏúÑ', 'ÏïÑÎûò', 'Ïïû', 'Îí§', 'ÏòÜ', 'ÏÇ¨Ïù¥',
    // Enhanced stopwords - common terms
    'Ï†úÌíà', 'ÏÉÅÌíà', 'Î∏åÎûúÎìú', 'ÌöåÏÇ¨', 'ÏóÖÏ≤¥', 'ÏÇ¨Ïù¥Ìä∏', 'ÌôàÌéòÏù¥ÏßÄ', 'ÏÑúÎπÑÏä§', 'Ïñ¥Ìîå', 'Ïï±',
    'Í∞ÄÍ≤©', 'ÎπÑÏö©', 'Í∏àÏï°', 'ÏöîÍ∏à', 'Ìï†Ïù∏', 'ÏÑ∏Ïùº', 'ÌäπÍ∞Ä', 'Î¨¥Î£å', 'Ïú†Î£å',
    'Íµ¨Îß§', 'Ï£ºÎ¨∏', 'Í≤∞Ï†ú', 'Î∞∞ÏÜ°', 'ÌÉùÎ∞∞', 'Ìè¨Ïû•', 'Î∞òÌíà', 'ÍµêÌôò', 'ÌôòÎ∂à',
    'Ïù¥Ïö©', 'ÏÇ¨Ïö©', 'ÌôúÏö©', 'Ï†ÅÏö©', 'ÏÑ§Ïπò', 'Îã§Ïö¥Î°úÎìú', 'ÏóÖÎç∞Ïù¥Ìä∏', 'Î≤ÑÏ†Ñ',
    // Blog/review specific stopwords  
    'Î∏îÎ°úÍ∑∏', 'Ìè¨Ïä§ÌåÖ', 'Î¶¨Î∑∞', 'ÌõÑÍ∏∞', 'Ï∂îÏ≤ú', 'ÏÜåÍ∞ú', 'Ï†ïÎ≥¥', 'Í≥µÏú†', 'Ïù¥ÏïºÍ∏∞', 'Í≤ΩÌóò', 'Î∞©Î≤ï', 'ÏÉùÍ∞Å', 'ÎäêÎÇå',
    'ÏÇ¨ÏßÑ', 'Ïù¥ÎØ∏ÏßÄ', 'ÏòÅÏÉÅ', 'ÎèôÏòÅÏÉÅ', 'ÎßÅÌÅ¨', 'ÎåìÍ∏Ä', 'Ï¢ãÏïÑÏöî', 'Íµ¨ÎèÖ', 'ÌåîÎ°úÏö∞', 'Í≤åÏãúÎ¨º', 'Í∏Ä', 'ÎÇ¥Ïö©',
    'Ïû•Ï†ê', 'Îã®Ï†ê', 'ÌäπÏßï', 'Ìö®Í≥º', 'Í∏∞Îä•', 'ÏÑ±Îä•', 'ÌíàÏßà', 'ÎîîÏûêÏù∏', 'Ïä§ÌÉÄÏùº', 'Ïª¨Îü¨', 'ÏÇ¨Ïù¥Ï¶à',
    // Generic qualifiers that add no value
    'ÏßÑÏßú', 'Ï†ïÎßê', 'ÏôÑÏ†Ñ', 'ÎÑàÎ¨¥', 'Ï†ïÎßêÎ°ú', 'Îß§Ïö∞', 'ÏïÑÏ£º', 'ÎßéÏù¥', 'Ï°∞Í∏à', 'ÏïΩÍ∞Ñ', 'Ï¢Ä', 'Îçî', 'Îçú',
    'ÏµúÍ≥†', 'ÏµúÏÉÅ', 'ÏµúÏã†', 'ÏµúÏ†Å', 'ÏµúÎåÄ', 'ÏµúÏÜå', 'ÏµúÏ†Ä', 'Ï≤´Î≤àÏß∏', 'ÎëêÎ≤àÏß∏', 'ÎßàÏßÄÎßâ',
    // Time/frequency terms
    'Ìï≠ÏÉÅ', 'Îäò', 'ÏûêÏ£º', 'Í∞ÄÎÅî', 'ÎïåÎ°úÎäî', 'Ïù¥Î≤à', 'Îã§Ïùå', 'ÏßÄÎÇú', 'Ïò§Îäò', 'Ïñ¥Ï†ú', 'ÎÇ¥Ïùº', 'ÏöîÏ¶ò', 'ÏµúÍ∑º',
    // Numbers and pure punctuation
    ...Array.from({length: 100}, (_, i) => i.toString()),
  ]);

  // Enhanced quality filters for better token refinement
  private isQualityKeyword(keyword: string): boolean {
    // Reject single characters only
    if (keyword.length <= 1) return false;
    
    // Reject pure numbers or numbers with simple Korean particles
    if (/^\d+[Í∞ÄÎÇòÎã§ÎùºÎßàÎ∞îÏÇ¨ÏïÑÏûêÏ∞®Ïπ¥ÌÉÄÌååÌïò]?$/.test(keyword)) return false;
    
    // Reject patterns with too many special characters (but allow Korean)
    const alnumKo = keyword.match(/[A-Za-z0-9Í∞Ä-Ìû£]/g) || [];
    if (alnumKo.length < Math.ceil(keyword.length * 0.6)) return false;
    
    // Reject very generic patterns
    const genericPatterns = [
      /^.$/, // Single character only (allow 2+ chars)
      /^[„Ñ±-„Öé„Öè-„Ö£]+$/, // Only Korean consonants/vowels
      /^\d+Ïõî$/, /^\d+Ïùº$/, /^\d+ÎÖÑ$/, // Pure date components
      /^[!@#$%^&*()]+$/, // Pure punctuation
    ];
    
    return !genericPatterns.some(pattern => pattern.test(keyword));
  }

  // Unified normalization for consistent matching
  private normalizeForMatch(text: string): string {
    return this.cleanText(text).toLowerCase().replace(/\s+/g, '');
  }

  /**
   * Clean and normalize text for processing
   */
  private cleanText(text: string): string {
    return text
      .replace(/<[^>]*>/g, '') // Remove HTML tags
      .replace(/[^\w\sÍ∞Ä-Ìû£]/g, ' ') // Keep only Korean, alphanumeric and spaces
      .replace(/\s+/g, ' ') // Normalize whitespace
      .trim()
      .toLowerCase();
  }

  /**
   * Extract n-grams from tokenized text
   * Supports 1-gram, 2-gram, and 3-gram extraction as specified
   */
  private extractNgrams(text: string, n: number): Array<{surface: string, normalized: string}> {
    const words = text.split(' ').filter(word => 
      word.length >= 2 && 
      !this.stopWords.has(word) &&
      !/^\d+$/.test(word) // Skip pure numbers
    );
    
    const ngrams: Array<{surface: string, normalized: string}> = [];
    
    for (let i = 0; i <= words.length - n; i++) {
      const surface = words.slice(i, i + n).join(' ');
      const normalized = this.normalizeForMatch(surface); // Unified normalization
      
      if (surface.length >= 2 && this.isQualityKeyword(surface)) {
        ngrams.push({ surface, normalized });
      }
    }
    
    return ngrams;
  }

  /**
   * Extract keywords using n-gram (1-3) approach from titles with keyword DB integration
   * This implements n-gram keyword extraction enhanced with existing keyword database
   */
  async extractKeywords(titles: string[]): Promise<KeywordCandidate[]> {
    console.log(`üî§ Starting n-gram keyword extraction from ${titles.length} titles`);
    
    // Get keyword volume map from database
    console.log(`üîç Fetching keywords from database...`);
    const dbKeywords = await listKeywords({ excluded: false, orderBy: 'raw_volume', dir: 'desc' });
    const keywordVolumeMap: Record<string, { volume: number; grade: string }> = {};
    
    dbKeywords.forEach(keyword => {
      // Normalize DB keyword text using unified normalization
      const normalizedText = this.normalizeForMatch(keyword.text);
      keywordVolumeMap[normalizedText] = {
        volume: keyword.raw_volume || keyword.volume || 0,
        grade: keyword.grade || 'C'
      };
    });
    
    const dbKeywordCount = dbKeywords.length;
    console.log(`üìä Loaded ${dbKeywordCount} keywords from database`);
    
    const keywordFreq = new Map<string, number>();
    
    // Clean titles and extract text
    const cleanedTitles = titles.map(title => this.cleanText(title));
    console.log(`üìù Cleaned titles: ${cleanedTitles.slice(0, 3).join(', ')}...`);
    
    // Extract 1-grams, 2-grams, and 3-grams with surface/normalized separation
    const ngramData = new Map<string, {surface: string, frequency: number}>();
    
    for (const title of cleanedTitles) {
      // 1-grams (single words)
      const unigrams = this.extractNgrams(title, 1);
      // 2-grams (word pairs)
      const bigrams = this.extractNgrams(title, 2);
      // 3-grams (word triplets)
      const trigrams = this.extractNgrams(title, 3);
      
      // Count frequency with surface/normalized separation and quality filtering
      [...unigrams, ...bigrams, ...trigrams].forEach(({surface, normalized}) => {
        const weight = surface.split(' ').length; // 1, 2, or 3
        
        // Use normalized for deduplication, but keep surface for display
        if (ngramData.has(normalized)) {
          const existing = ngramData.get(normalized)!;
          existing.frequency += weight;
        } else {
          ngramData.set(normalized, {
            surface, // Keep original surface form
            frequency: weight
          });
        }
      });
    }
    
    // Calculate relevance scores with DB integration
    const candidates: KeywordCandidate[] = [];
    const totalTitles = titles.length;
    
    for (const [normalized, {surface, frequency}] of Array.from(ngramData.entries())) {
      if (frequency >= 2 && this.isQualityKeyword(surface)) { // Enhanced quality filtering
        // Document frequency: how many titles contain this keyword (using normalized form for consistency)
        const normalizedTitles = cleanedTitles.map(title => this.normalizeForMatch(title));
        const documentFreq = normalizedTitles.filter(title => 
          title.includes(normalized)
        ).length;
        
        // Check if keyword exists in database (using unified normalized lookup)
        const dbKeywordInfo = keywordVolumeMap[normalized];
        const isFromDB = !!dbKeywordInfo;
        
        // Base score calculation (using surface form for length)
        let score = Math.round(
          (frequency * 10) + // Base frequency score
          (surface.split(' ').length * 5) + // Length bonus
          (documentFreq / totalTitles * 20) // Document frequency bonus
        );
        
        // Boost score for keywords found in database
        if (isFromDB) {
          const volumeBoost = Math.min(dbKeywordInfo.volume / 1000, 50); // Volume-based bonus (max 50)
          const gradeBoost = dbKeywordInfo.grade === 'A' ? 30 : dbKeywordInfo.grade === 'B' ? 20 : 10;
          score += Math.round(volumeBoost + gradeBoost);
          
          console.log(`üéØ DB keyword found: "${surface}" (volume: ${dbKeywordInfo.volume}, grade: ${dbKeywordInfo.grade}, boost: +${Math.round(volumeBoost + gradeBoost)})`);
        }
        
        candidates.push({
          keyword: surface, // Use surface form for display
          frequency,
          score,
          volume: dbKeywordInfo?.volume,
          grade: dbKeywordInfo?.grade,
          isFromDB,
        });
      }
    }
    
    // Sort by score and return top candidates
    const sortedCandidates = candidates
      .sort((a, b) => b.score - a.score)
      .slice(0, 50); // Return top 50 keywords
    
    console.log(`‚úÖ N-gram extraction complete: ${sortedCandidates.length} keyword candidates found`);
    console.log(`üèÜ Top 5 keywords: ${sortedCandidates.slice(0, 5).map(k => `${k.keyword}(${k.score})`).join(', ')}`);
    
    return sortedCandidates;
  }

  /**
   * Get top N keywords from candidates
   */
  getTopKeywords(candidates: KeywordCandidate[], limit = 20): KeywordCandidate[] {
    const topKeywords = candidates.slice(0, limit);
    console.log(`üìä Selected top ${topKeywords.length} keywords for analysis`);
    return topKeywords;
  }
}

export const nlpService = new NLPService();