export interface KeywordCandidate {
  keyword: string;
  frequency: number;
  score: number;
}

export class NLPService {
  private stopWords = new Set([
    // Korean stopwords
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ë“¤', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë„', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì´ë‹¤', 'ìžˆë‹¤', 'í•˜ë‹¤',
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜', 'ê·¸ëž˜ì„œ', 'ë”°ë¼ì„œ', 'ì¦‰', 'ì˜ˆë¥¼ ë“¤ì–´', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´',
    'ì •ë§', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì •ë§ë¡œ', 'í•­ìƒ', 'ëŠ˜', 'ìžì£¼', 'ê°€ë”', 'ë•Œë¡œëŠ”', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
    'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³', 'ìœ„', 'ì•„ëž˜', 'ì•ž', 'ë’¤', 'ì˜†', 'ì‚¬ì´',
    // Common blog words
    'ë¸”ë¡œê·¸', 'í¬ìŠ¤íŒ…', 'ë¦¬ë·°', 'í›„ê¸°', 'ì¶”ì²œ', 'ì†Œê°œ', 'ì •ë³´', 'ê³µìœ ', 'ì´ì•¼ê¸°', 'ê²½í—˜', 'ë°©ë²•', 'ìƒê°', 'ëŠë‚Œ',
    'ì‚¬ì§„', 'ì´ë¯¸ì§€', 'ì˜ìƒ', 'ë™ì˜ìƒ', 'ë§í¬', 'ëŒ“ê¸€', 'ì¢‹ì•„ìš”', 'êµ¬ë…', 'íŒ”ë¡œìš°',
    // Numbers and punctuation patterns
    ...Array.from({length: 100}, (_, i) => i.toString()),
  ]);

  /**
   * Clean and normalize text for processing
   */
  private cleanText(text: string): string {
    return text
      .replace(/<[^>]*>/g, '') // Remove HTML tags
      .replace(/[^\w\sê°€-íž£]/g, ' ') // Keep only Korean, alphanumeric and spaces
      .replace(/\s+/g, ' ') // Normalize whitespace
      .trim()
      .toLowerCase();
  }

  /**
   * Extract n-grams from tokenized text
   * Supports 1-gram, 2-gram, and 3-gram extraction as specified
   */
  private extractNgrams(text: string, n: number): string[] {
    const words = text.split(' ').filter(word => 
      word.length >= 2 && 
      !this.stopWords.has(word) &&
      !/^\d+$/.test(word) // Skip pure numbers
    );
    
    const ngrams: string[] = [];
    
    for (let i = 0; i <= words.length - n; i++) {
      const ngram = words.slice(i, i + n).join(' ');
      if (ngram.length >= 2) {
        ngrams.push(ngram);
      }
    }
    
    return ngrams;
  }

  /**
   * Extract keywords using simple n-gram (1-3) approach from titles
   * This implements the specified simple n-gram keyword extraction
   */
  extractKeywords(titles: string[]): KeywordCandidate[] {
    console.log(`ðŸ”¤ Starting n-gram keyword extraction from ${titles.length} titles`);
    
    const keywordFreq = new Map<string, number>();
    
    // Clean titles and extract text
    const cleanedTitles = titles.map(title => this.cleanText(title));
    console.log(`ðŸ“ Cleaned titles: ${cleanedTitles.slice(0, 3).join(', ')}...`);
    
    // Extract 1-grams, 2-grams, and 3-grams as specified
    for (const title of cleanedTitles) {
      // 1-grams (single words)
      const unigrams = this.extractNgrams(title, 1);
      // 2-grams (word pairs)
      const bigrams = this.extractNgrams(title, 2);
      // 3-grams (word triplets)
      const trigrams = this.extractNgrams(title, 3);
      
      // Count frequency with different weights
      // Higher weight for longer phrases as they're more specific
      [...unigrams, ...bigrams, ...trigrams].forEach(ngram => {
        const weight = ngram.split(' ').length; // 1, 2, or 3
        keywordFreq.set(ngram, (keywordFreq.get(ngram) || 0) + weight);
      });
    }
    
    // Calculate relevance scores
    const candidates: KeywordCandidate[] = [];
    const totalTitles = titles.length;
    
    for (const [keyword, frequency] of Array.from(keywordFreq.entries())) {
      if (frequency >= 2) { // Filter out keywords that appear only once
        // Document frequency: how many titles contain this keyword
        const documentFreq = cleanedTitles.filter(title => 
          title.includes(keyword)
        ).length;
        
        // Score based on frequency, length, and document frequency
        const score = Math.round(
          (frequency * 10) + // Base frequency score
          (keyword.split(' ').length * 5) + // Length bonus
          (documentFreq / totalTitles * 20) // Document frequency bonus
        );
        
        candidates.push({
          keyword,
          frequency,
          score,
        });
      }
    }
    
    // Sort by score and return top candidates
    const sortedCandidates = candidates
      .sort((a, b) => b.score - a.score)
      .slice(0, 50); // Return top 50 keywords
    
    console.log(`âœ… N-gram extraction complete: ${sortedCandidates.length} keyword candidates found`);
    console.log(`ðŸ† Top 5 keywords: ${sortedCandidates.slice(0, 5).map(k => `${k.keyword}(${k.score})`).join(', ')}`);
    
    return sortedCandidates;
  }

  /**
   * Get top N keywords from candidates
   */
  getTopKeywords(candidates: KeywordCandidate[], limit = 20): KeywordCandidate[] {
    const topKeywords = candidates.slice(0, limit);
    console.log(`ðŸ“Š Selected top ${topKeywords.length} keywords for analysis`);
    return topKeywords;
  }
}

export const nlpService = new NLPService();